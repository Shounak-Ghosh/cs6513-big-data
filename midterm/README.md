# Question 1
I used one mapper and a two-stage reducer for scalable text analysis. The mapper normalizes input by converting words to lowercase, removing punctuation, and filtering out stop words, then emits two key-value pairs: (WORD, word) for frequency counting and (LENGTH, length) for length distribution. The first reducer performs streaming aggregation over sorted input to count word and length occurrences without storing data in memory, outputting intermediate results as WORD <word> <count> and LENGTH <length> <count>. The second reducer merges these partial aggregates across all reducers to compute global word frequencies, unique and total word counts, and the complete word length distribution, producing the final summarized output efficiently.